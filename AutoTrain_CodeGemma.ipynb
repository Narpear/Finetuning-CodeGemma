{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyONXmpdeEA4GGcrEwIn+ecw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"HOcSFUvClMxa","executionInfo":{"status":"ok","timestamp":1716718324521,"user_tz":-330,"elapsed":168195,"user":{"displayName":"Prerana Kulkarni","userId":"13041264337790910045"}}},"outputs":[],"source":["#@title ðŸ¤— AutoTrain LLM\n","#@markdown In order to use this colab\n","#@markdown - upload train.csv to a folder named `data/`\n","#@markdown - train.csv must contain a `text` column\n","#@markdown - choose a project name if you wish\n","#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n","#@markdown - add huggingface information (token) if you wish to push trained model to huggingface hub\n","#@markdown - update hyperparameters if you wish\n","#@markdown - click `Runtime > Run all` or run each cell individually\n","#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n","\n","import os\n","!pip install -U autotrain-advanced > install_logs.txt\n","!autotrain setup --colab --update-torch> setup_logs.txt"]},{"cell_type":"code","source":["#@markdown ---\n","#@markdown #### Project Config\n","#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n","project_name = 'hpecty' # @param {type:\"string\"}\n","model_name = 'google/codegemma-7b' # @param {type:\"string\"}\n","\n","#@markdown ---\n","#@markdown #### Push to Hub?\n","#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n","#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n","#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n","#@markdown You can find your token here: https://huggingface.co/settings/tokens\n","push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n","hf_token = \"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\" #@param {type:\"string\"}\n","hf_username = \"Narpear\" #@param {type:\"string\"}\n","\n","#@markdown ---\n","#@markdown #### Hyperparameters\n","learning_rate = 2e-4 # @param {type:\"number\"}\n","num_epochs = 1 #@param {type:\"number\"}\n","batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n","block_size = 1024 # @param {type:\"number\"}\n","trainer = \"sft\" # @param [\"default\", \"sft\", \"orpo\"] {type:\"raw\"}\n","warmup_ratio = 0.1 # @param {type:\"number\"}\n","weight_decay = 0.01 # @param {type:\"number\"}\n","gradient_accumulation = 4 # @param {type:\"number\"}\n","mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n","peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n","quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\n","lora_r = 16 #@param {type:\"number\"}\n","lora_alpha = 32 #@param {type:\"number\"}\n","lora_dropout = 0.05 #@param {type:\"number\"}\n","\n","os.environ[\"PROJECT_NAME\"] = project_name\n","os.environ[\"MODEL_NAME\"] = model_name\n","os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n","os.environ[\"HF_TOKEN\"] = hf_token\n","os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n","os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n","os.environ[\"BATCH_SIZE\"] = str(batch_size)\n","os.environ[\"BLOCK_SIZE\"] = str(block_size)\n","os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n","os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n","os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n","os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n","os.environ[\"PEFT\"] = str(peft)\n","os.environ[\"QUANTIZATION\"] = str(quantization)\n","os.environ[\"LORA_R\"] = str(lora_r)\n","os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n","os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n","os.environ[\"HF_USERNAME\"] = hf_username\n","os.environ[\"TRAINER\"] = trainer"],"metadata":{"id":"EYriMcYllVpo","executionInfo":{"status":"ok","timestamp":1716719688785,"user_tz":-330,"elapsed":426,"user":{"displayName":"Prerana Kulkarni","userId":"13041264337790910045"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["!autotrain llm \\\n","--train \\\n","--model ${MODEL_NAME} \\\n","--project-name ${PROJECT_NAME} \\\n","--data-path /content/data/ \\\n","--text-column text \\\n","--lr ${LEARNING_RATE} \\\n","--batch-size ${BATCH_SIZE} \\\n","--epochs ${NUM_EPOCHS} \\\n","--block-size ${BLOCK_SIZE} \\\n","--warmup-ratio ${WARMUP_RATIO} \\\n","--lora-r ${LORA_R} \\\n","--lora-alpha ${LORA_ALPHA} \\\n","--lora-dropout ${LORA_DROPOUT} \\\n","--weight-decay ${WEIGHT_DECAY} \\\n","--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n","--quantization ${QUANTIZATION} \\\n","--mixed-precision ${MIXED_PRECISION} \\\n","--username ${HF_USERNAME} \\\n","--trainer ${TRAINER} \\\n","$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n","$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN}\" )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3eeQZlRlsJd","executionInfo":{"status":"ok","timestamp":1716719721983,"user_tz":-330,"elapsed":30248,"user":{"displayName":"Prerana Kulkarni","userId":"13041264337790910045"}},"outputId":"e004a09a-d220-479e-aefc-4d694b9faf60"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:34:59\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n","\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-26 10:34:59\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: backend, inference, deploy, train, version, func, config\u001b[0m\n","\rSaving the dataset (0/1 shards):   0% 0/50 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 50/50 [00:00<00:00, 6952.73 examples/s]\rSaving the dataset (1/1 shards): 100% 50/50 [00:00<00:00, 6708.10 examples/s]\n","\rSaving the dataset (0/1 shards):   0% 0/50 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 50/50 [00:00<00:00, 12692.32 examples/s]\rSaving the dataset (1/1 shards): 100% 50/50 [00:00<00:00, 12214.76 examples/s]\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:34:59\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n","\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-26 10:34:59\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m43\u001b[0m - \u001b[33m\u001b[1mNo GPU found. Forcing training on CPU. This will be super slow!\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:34:59\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m372\u001b[0m - \u001b[1m['accelerate', 'launch', '--cpu', '-m', 'autotrain.trainers.clm', '--training_config', 'hpecty/training_params.json']\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:34:59\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1m{'model': 'google/codegemma-7b', 'project_name': 'hpecty', 'data_path': 'hpecty/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 1024, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': True, 'username': 'Narpear', 'token': '*****'}\u001b[0m\n","The following values were not passed to `accelerate launch` and had defaults used instead:\n","\t`--num_processes` was set to a value of `0`\n","\t`--num_machines` was set to a value of `1`\n","\t`--mixed_precision` was set to a value of `'no'`\n","\t`--dynamo_backend` was set to a value of `'no'`\n","To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n","No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:15\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:15\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m329\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:15\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m388\u001b[0m - \u001b[1mTrain data: Dataset({\n","    features: ['instruction', 'input', 'output', 'autotrain_text'],\n","    num_rows: 50\n","})\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:15\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m389\u001b[0m - \u001b[1mValid data: None\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m461\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mLogging steps: 10\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m479\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m542\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mloading model config...\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mloading model...\u001b[0m\n","\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/common.py\", line 117, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 28, in train\n","    train_sft(config)\n","  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/train_clm_sft.py\", line 49, in train\n","    model = AutoModelForCausalLM.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n","    return model_class.from_pretrained(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3202, in from_pretrained\n","    hf_quantizer.validate_environment(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 62, in validate_environment\n","    raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n","RuntimeError: No GPU found. A GPU is needed for quantization.\n","\u001b[0m\n","\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-05-26 10:35:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mNo GPU found. A GPU is needed for quantization.\u001b[0m\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-26 10:35:21\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 22135\u001b[0m\n"]}]}]}